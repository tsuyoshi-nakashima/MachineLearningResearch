- Abstract
  - 物体姿勢復元は、自律走行、ロボット工学、拡張現実などに関連する急速に発展する技術分野において重要な問題になっているため、コンピュータビジョン分野でますます注目を集めています。既存のレビュー関連の研究では、RGB画像中の注目オブジェクトの2Dバウンディングボックスを生成する方法を経て、2Dの視覚レベルでこの問題に取り組んでいる。2次元の探索空間は、RGB（モノ／ステレオ）画像と3次元空間の幾何情報を利用するか、LIDARセンサーやRGB-Dカメラからの奥行きデータを利用して拡大される。3次元バウンディングボックス検出器は、カテゴリレベルのアモーダルな3次元バウンディングボックスを生成し、重力アライメント画像で評価されるのに対し、完全な6次元物体姿勢推定器は、アライメント制約を除去した画像でインスタンスレベルでテストされることがほとんどである。近年、6次元物体姿勢推定はカテゴリレベルで取り組まれている。本論文では、3次元バウンディングボックス検出器から完全な6次元姿勢推定器まで、物体姿勢復元に関する手法の包括的かつ最新のレビューを初めて提示する。これらの手法は、問題を分類、回帰、分類と回帰、テンプレートマッチング、点対特徴量マッチングのタスクとして数学的にモデル化する。これに基づき、数学的モデルに基づく手法の分類が確立される。手法の評価に用いるデータセットについて、課題との関連性を調査し、評価指標を検討する。文献にある定量的な実験結果を分析し、どのカテゴリーの手法がどのような種類の課題に対して最も良い性能を発揮するかを示す。さらに、公開されている結果をより確かなものにするために、私たち自身の実装である2つの手法を比較し、分析を拡張しています。オブジェクトの姿勢回復に関して、この分野の現在の位置を要約し、可能な研究の方向性を明らかにする。

 - 物体の姿勢復元に関する研究を進める中で、2次元の視覚レベルにおける物体の姿勢復元を扱ったいくつかのレビュー関連論文[8,9,10,11,12,13,109,199]を目にした。これらの論文では、オクルージョン、クラッタ、テクスチャなどの課題が手法の性能に与える影響について議論されており、主にImageNet [6] やPASCAL [7] などの大規模データセットで評価されています。8]では、地理的コンテキスト、オブジェクトの空間的サポートなど、異なるコンテキストソースの2Dオブジェクト検出への影響が検討されている。Hoiemら[9]は、PASCALデータセットにおけるいくつかのベースラインの性能を評価し、特に偽陽性が仮定される理由を分析している。Torralbaら[11]は、関係するサンプル、データセット間の汎化、および相対的なデータの偏りに関して、いくつかのデータセットを比較しています。PASCALでは物体カテゴリが少ないため、Russakovskyら[10]はImageNetを用いてメタ分析を行い、色やテクスチャなどが物体検出器の性能に与える影響について調べています。また、PASCAL Visual Object Classes (VOC) ChallengeとImageNet Large Scale Visual Recognition Challengeをそれぞれ調査したretrospective evaluation [12] と benchmark [13] の研究は、2Dオブジェクトローカリゼーションとカテゴリ検出に関する最も包括的な分析を行っています。最近提案された研究[109]は、顕著なオブジェクト検出、顔検出、歩行者検出などのいくつかの特定のタスクを含む深層学習ベースのオブジェクト検出フレームワークを体系的にレビューしています。これらの研究は、一般化されたオブジェクト検出のための重要な意味を導入しているが、レビューされた方法と議論は、2Dの視覚レベルに限定されている。2D駆動型3Dバウンディングボックス（BB）検出法は、RGB画像とともに3D空間の利用可能な外観・幾何学情報を用いて2D探索空間を拡大する[90,151,153,154,144]。84,155]では、単眼RGB画像中の物体の3次元BBを、文脈モデルやセマンティクスを利用して直接検出する方法を示している。84,155]以外では、ステレオ画像[92]、RGB-Dカメラ[86,93,94,95,152]、LIDARセンサ[152]を入力として直接3D BBを検出する手法がある。また、複数のセンサ入力を融合し、3次元BBの仮説を生成する手法もいくつかある[107]。3次元BB検出法は、入力（RGB（Mo／St）、RGB-D、LIDAR）に依存せず、中心x＝（x，y，z）、サイズd＝（dw，dh，dl）、重力方向周りの向き（θy）でパラメータ化した配向性3次元BBを生成します。3次元BB検出器は6次元空間に拡張可能であるが、文献にある手法は主にKITTI [110], NYU-Depth v2 [111], SUN RGB-D [112] データセットで評価されており、関心対象が重力方向に整列しているものであることに注意。これらの手法は、データセットが要求するように、自動車、自転車、歩行者、椅子、ベッドなどのカテゴリレベルで動作し、アモーダル3D BB、すなわち、関心のあるオブジェクトを囲む最小の3D BBを生成する。

  - 物体の姿勢を復元するために考案された手法の探索空間は、さらに6次元に拡大され[165,171,175,183]、すなわち、3次元並進x = (x,y,z) と3次元回転 Θ = (θr,θp,θy) である。また、RGB-D画像から注目物体の6次元姿勢を推定する手法もある。全方位型テンプレートマッチング手法であるLinemod[2]は、色勾配と表面法線を用いて、乱雑な物体の6次元姿勢を推定するものである。この手法は、[24]において、識別学習により改良されている。FDCM [23]はロボット工学の応用に用いられている．Drostら[17]は、配向点対の特徴に基づくグローバルなモデル記述を作成し、高速な投票スキームを用いてそのモデルを局所的にマッチングさせる。さらに[43]では、この方法をクラッタやセンサノイズに対してより頑健にするために改良されている。潜在クラスハフフォレスト（LCHF）[4,26]は、1クラス学習を採用し、オクルージョンに対してロバスト性を提供するために、パーツベースのアプローチで表面の法線と色の勾配特徴を利用します。さらに、オクルージョンを考慮した特徴量[27]が定式化されている。また，[28,29]では，テクスチャのない物体を扱っている．さらに最近では、深層回路網（net）を用いて教師無しで特徴表現を学習する2 C. Sahin et al.／Image and Vision Computing 96 (2020) 103898 [35,36]. これらの手法は、RGBチャンネルと深度チャンネルからのデータを融合しているが、深度モダリティにおいては、局所的な信念伝搬に基づくアプローチ[73]と反復的洗練アーキテクチャ[31,32]が提案されている[74]。近年、RGB のみから 6 次元物体姿勢推定が実現されており[164,170,172,173,174,30,37,38,40]、現在のパラダイムは CNN の採用である [157,158,169]．BB8[40]やTekinら[38]は、角点回帰の後にPnPを行う。一般的には、反復最近点（ICP）や検証ネット（37）など、計算量の多い後処理が採用される。主にLINEMOD [2], OCCLUSION [28], LCHF [4], TLESS [42] データセットで評価されているように、完全6次元物体姿勢推定法は通常インスタンスのレベルで動作する。しかし、最近提案された手法[113,114]は、カテゴリレベルで6次元物体姿勢推定問題に取り組み、例えば、分布シフト、クラス内変動などの課題を扱っている[206]。本研究では、3次元BB検出器から完全な6次元姿勢推定器までの手法をレビューし、物体姿勢回復に関する包括的なレビューを行う。レビューされた手法は、分類、回帰、分類と回帰、テンプレートマッチング、および点対特徴マッチングタスクとして、オブジェクトの姿勢回復を数学的にモデル化しています。これに基づいて、数学的モデルに基づく手法の分類が確立され、5つの異なるカテゴリが形成される。さらに、各カテゴリ間の曖昧さをなくすため、進歩や欠点について研究している。文献にある個々のデータセットには、ある種の課題が含まれており、それに対して手法をテストすることで性能を測定することができる。視点変動、オクルージョン、クラッタ、クラス内変動、分布シフトなどの問題の課題は、オブジェクトの姿勢回復のために作成されたデータセットを調査することによって特定される。さらに、手法の性能を評価するために使用されるプロトコルについても検討する。レビューされた手法の分類を紹介し、問題の課題を特定したら、次に、課題に対する手法の性能を明らかにする。この目的のために、平均距離（AD）メトリック[2]の均一な採点基準の下で計算された、一般に入手可能なリコール値を分析する。さらに、ADに加えてVisible Surface Discrepancy (VSD)プロトコル[3]を使用した当社独自の実装である2つの手法[2,4]を比較して分析を拡張しています。この拡張は主に、公開された結果から得られた成果を活用し、この問題のあらゆる側面をつなぐ議論を完了させることを目的としています。最後に、物体姿勢回復に関するこの分野の現在の位置をまとめ、可能な研究の方向性を明らかにする。

  物体姿勢復元問題に対する手法は、物体姿勢パラメータである3次元移動x＝（x,y,z）と／3次元回転Θ＝（θr,θp,θy）を推定するものである。本論文では、問題のモデル化アプローチに基づいて、レビューされた方法を分類する。問題を分類タスクとして定式化し、ポーズパラメータを推定する手法は分類に含まれ、パラメータを回帰する手法は回帰に含まれる。分類と回帰は、分類と回帰の両方のタスクを組み合わせて、物体の3次元移動と3次元回転を推定する。テンプレートマッチング法は、特徴空間において、注釈付きテンプレートと表現されたテンプレートに一致する物体の姿勢パラメータを推定する方法である。点対特徴マッチング法は、任意の2点間の距離や法線間の角度などの関係を用いて、点対を表現する方法です。また、ハッシュテーブルと効率的な投票方式により、対象物体の姿勢パラメータを予測する。これらの5つの異なるモデリングアプローチ、分類、回帰、分類と回帰、テンプレートマッチング、点対特徴マッチングは、3Dから6Dまでのオブジェクト姿勢回復問題に対する最先端の手法をレビューするための識別的分類を形成しています。これらの手法を検討すると、2次元駆動型の3次元検出手法に遭遇することがある。2D駆動型3D手法は、例えば、R-CNN [115]、Fast R-CNN [116]、Faster RCNN [117]、R-FCN [118]、FPN [119]、YOLO [120]、SSD [121] 、GOP [122] またはMS-CNN [123] といった既製の2D検出器を利用してまず関心対象の2D BBs [156] を検出し、続いてこれを3D空間へ持ち上げ、したがってその性能が2D検出器に依存していることがわかる。さらに、その上に、オブジェクトの3次元並進／回転パラメータを直接生成する、いくつかの3次元検出器と完全な6次元姿勢推定器が構築されている。決定森（Decision Forest）[127]は、オブジェクトの姿勢回復の問題に対して重要である。手法の分類に入る前に、まず、いくつかの2次元検出器と決定森について簡単に言及する。R-CNN [115]は、入力RGB画像から、ボトムアップのグループ化と顕著性の手がかりに依存する選択的探索スキーム[124]を採用して、任意のサイズの領域提案の束を生成する。領域案は一定の解像度にワープされ、[125]のCNNアーキテクチャに供給され、CNN特徴ベクトルで表現される。各特徴ベクトルに対して、カテゴリ固有のSVMは、その領域がさらにBB回帰、および非最大抑制（NMS）フィルタリングで調整されるスコアを生成する。この3段階の処理により、R-CNNは、DPM HSC [126]などの従来の最良の2次元手法に対して約30%の改善を達成することができる。Fast R-CNN [116]は、RGB画像と領域提案の集合を入力とする。画像全体は、conv特徴マップを生成するために処理され、そこから固定長の特徴ベクトルが、各領域提案に対して生成される。各特徴ベクトルは、一連の完全連結（FC）層に供給され、最終的にソフトマックス分類とBB回帰の出口に分岐される。それぞれの学習用関心領域(RoI)には、真値クラスと真値回帰目標がラベル付けされる。Fast RCNNは各RoIに対してマルチタスク損失を導入し、分類とBB回帰のための共同学習を行う。

https://qiita.com/arutema47/items/8ff629a1516f7fd485f9

# 物体検出器のお話

## 大まかな流れ
物体がありそうな領域に対してCNNを回して物体認識する。CNN自体に時間がかかるからできる限りCNNの回数を減らしたい。

## R-CNN

## Fast-RCNN
R-CNNではROIに対して毎回CNNを回さなければいけなかった。
（流れ：RoIの算出→CNNで特徴マップ出力→クラス・BB算出）
一方でFast-RCNNは特徴マップとROIを入力としてクラス・BB算出をするため、CNNを回す回数が一回で済む。
（流れ：領域全体にCNNを回して特徴マップ取得。ROIは特徴マップに対して設定　→　FCLayerによってクラス・BBを算出）
ただしRegionProposalに時間がかかる問題は解決されておらず、大半がRegionProporsalに計算時間を要するため、RoI算出がボトルネックであることは変わらず。

## Faster-RCNN
RoIの算出をSelectiveSearchからCNNによる出力を入力としてRoIを抽出するモデルを追加した。RoIを提案するネットワークをRPN（RegionProposalNetwork）と呼んでいる。RoIの抽出にもCNNを使用することで速度改善を図ることに成功した。

## YOLO
Faster-RCNNではRPNでの計算自体が全体の計算処理におけるボトルネックになり、処理時間の遅延を招いている。（検出→識別を逐次処理で実行しているから）
YOLOでは検出と識別を同時に行うことがモチベーションになっている。（単一のネットワークアーキテクチャでE2Eで推論を実行する）
流れ
①画像をS×Sのグリッドセルに分割し、各グリッドセル内でB個のBoundingBoxを推定とConfidennceを算出
②また各グリッドセルの物体クラスそれぞれの条件付きクラス確立を算出
①、②の結果から信頼度スコアを算出し、どのBBoxが対象とするクラスの物体を正確に検出しているかを判断する。

## SSD
https://www.acceluniverse.com/blog/developers/2020/02/SSD.html

# 6DoF推定の話
## トレーニング方法
オフラインの段階で、分類器は合成データまたは実データに基づいて学習される。合成データは、対象物Oの3次元モデルMと、異なるカメラ視点からのRGB／D／RGB-D画像群を用いて生成される。3次元モデルMはCADでも再構成モデルでもよく、データの大きさを決める際には以下のような要素を考慮する。
- 合理的な視点カバー率。対象物の合理的な視点範囲を捉えるために、半径が一定で細分化された正20面体の各頂点に仮想カメラを配置して合成画像を作成する。シナリオに応じて、正20面体の半球体や全球体を使用することができる。
- オブジェクトの距離 合成画像は、対象物の位置する範囲によって異なるスケールでレンダリングされます。
 
コンピュータグラフィックシステムは、正確なデータアノテーションを提供するため、分類ベースの手法では、これらのシステムで生成された合成データが使用される[88,81,89,27,28,29,33]。しかし、実写の画像に対して正確な物体の姿勢のアノテーションを得ることは困難であり、実写の学習データを用いた分類ベースの手法が存在する[28,29,33,77,88,89,96,104,149]。学習データは、姿勢パラメータ、すなわち、3次元移動x = (x,y,z), 3次元回転Θ = (θr,θp,θy)、またはその両方によって注釈される。学習データが生成されると、分類器は関連する損失関数を用いて学習される。

## 学習方法
オンライン段階での実際のテスト画像は、分類器による入力とされる。2D-driven3D法 [104,88,81] は、まず注目するオブジェクトの周囲の2D BBを抽出し（2D BB生成ブロック）、これを3Dにリフトアップさせる。入力によっては、[88,81,89,149,96,77,27]の方法は、入力画像に前処理を施し、3D仮説を生成する（入力前処理ブロック）。6次元物体姿勢推定器[27,28,29,33]は、入力画像から特徴を抽出し（特徴抽出ブロック）、学習した分類器を用いて、物体の6次元姿勢を推定する。さらに，学習された分類器の出力を洗練させ（洗練ブロック），フィルタリング後に最終的に物体の姿勢を推定する手法もある [104,81,149,28,29,33]．

表 2 は，分類に基づく手法の詳細を示している．GS3D [104] は、2D画像に隠された3D情報を抽出し、正確な3D BB仮説を生成することに特化している。この手法は、Faster R-CNN [117]を改良し、2次元BBパラメータに加えて、RGB画像における回転θyを分類している。別のCNNアーキテクチャを利用し、x、d、θyをさらに分類し、オブジェクトのポーズパラメータを洗練させる。Paponら[88]は、複雑な乱雑なシーンにおける一般的な家具クラスの意味的なポーズを推定する。入力は強度(RGB)と表面法線(D)画像に変換される。2次元GOP検出器[122]を用いて2次元BB提案を生成し、これをさらにビンベースのクロスエントロピー損失Lceを用いてθyとzを分類して3次元空間へ持ち上げる。Guptaら[81]は、[76]から出力されたセグメント化された領域から開始し、その深度チャンネルは、表面法線画像を取得するためにさらに処理される。セグメント化された領域の回転θyは、Lce損失によって分類される。本手法は、さらに、オブジェクトのCADモデル上でICPを採用し、推定された粗いポーズを洗練させる。SVM-based Sliding Shapes (SS)[89]の入力深度画像は、ボクセル化された点群を得るために前処理される。この分類器は、ヒンジ損失 Lhinge を用いて、パラメータ x、d、θy を分類するために学習される。Renら[149]は，2次元の外観を3次元点群にリンクさせる Cloud of Oriented Gradients（COG）記述子を提案している．この方法は、3Dボックス提案を生成し、そのパラメータx、d、θyは、NMSフィルタリング後、さらに別の分類器（s-SVM）により洗練され、最終的な検出値を得ることができる。Wangら[96]は、点群の疎な性質を利用し、すべての推定物体位置の検索を可能にする投票スキームを採用しています。これは、LIDAR画像を3Dグリッドに変換した後、sSVMを使用してパラメータxを推定するものである。Wangら[96]と同様に、Vote3Deep[77]の入力LIDAR画像は、スパース3Dグリッドを取得するために前処理され、xおよびθyパラメータを分類するためにCNNアーキテクチャに供給される。3次元BB検出器の探索空間は、さらに6次元に拡大される[27,28,29,33]。ランダムフォレストに基づくBondeらの手法[27]は、オクルージョン、クラッタ、および類似した外観のディストラクタが同時に存在する奥行き画像において、オブジェクトの6次元姿勢を推定する。この方法は，情報利得 IG に基づいて学習され，Θを分類する．この手法は，3次元回転Θを分類するにもかかわらず，エッジレット特徴を抽出した後，シーンをボクセル化し，エッジレット特徴間の距離が最小となるボクセルの中心を検出物体の中心として選択する．Brachmannら[28]は、6次元物体姿勢推定問題に対するランダムフォレストに基づく手法を紹介している。この方法は、Bondeら[27]とは異なり、ICP-variantアルゴリズムを用いて、正確な6Dポーズを得る、関心のあるオブジェクトの3D並進パラメータxを分類するものである。Brachmannら[28]に基づいて、Krullら[29]とMichelら[33]は、6Dオブジェクトポーズ回復のための新しいRGB-Dベースの方法を提示する。29]と[33]の主な貢献は、絞り込みのステップにある。Krull ら[29]は観察とレンダリングの比較を学習する新しい CNN アーキテクチャを学習し， Michel ら[33]はポーズ仮説のプールを生成する新しい条件付きランダムフィールド（CRF） を設計している．これらの手法の学習段階は、[104,149,96,77]は実データに基づいており、[81,27]は合成画像を使って学習している。Papon ら[88]は合成画像に基づいて手法を学習しますが，さらに分類器を微調整するために実際のポジティブデータを利用し，未経験のインスタンスに対する手法の精度を向上させるヒューリスティックな手法を採用しています．Sliding Shapes [89] は、SVM を学習するために、ポジティブな合成画像とネガティブな実画像を使用しています。Brachmann ら[28]，それに伴い Krull ら[29]と Michel ら[33]は，正と負の実データ，正の合成データに基づいてフォレストを学習している．このうち、[88,89,149,96,77]で示される手法の仮説は、NMSフィルタリングされている。2D駆動の3D手法と3D手法はすべてカテゴリのレベルで動作する。完全な6Dポーズ推定器は、インスタンスレベルのオブジェクトポーズ推定用に設計されている。

## 回帰
Fig. 2 は回帰分析法の全体像を示している。回帰手法の学習とテストの段階は、分類手法のそれと似ている。オフラインの段階では、関連する損失関数を用いて、注釈付きの実データまたは合成データに基づいて回帰器を学習する。オンライン段階では、実画像がいずれかの回帰器によって入力として扱われる。2D-driven3D法[101,85,83,147]は、まず、関心対象の周りの2D BBを抽出し（2D BB生成ブロック）、それを3Dにリフトアップしている。入力データを学習済みリグレッサに適したものにするために、いくつかの方法 [101,79,150,142,146,80,82,143,108] によって入力前処理 (input pre-processing block) が行われる。6次元物体姿勢推定器 [40,38,163,167,168,159,160,31,32,4,35,161] は、入力画像から特徴を抽出し、学習済みリグレッサを用いて、物体の6次元姿勢を推定する。さらに，学習されたリグレッサの出力を改良し [101,83,79,82,108,40,38,163,167,168,159,160,31,32,4,35,161] （改良ブロック），フィルタリング後に物体の姿勢を仮説化する手法もある．表 3 に回帰に基づく手法を詳しく示す．Wang ら[101]は、LIDAR ベースの手法と RGB ベースの手法の間の性能差は、データの質ではなく、表現にあると主張している。彼らは、RGB（Mo／St）から取得した画像ベースの深度マップを擬似LiDAR（p-LIDAR）表現に変換し、これを回帰ベースの検出アルゴリズムAVOD［108］の入力として与えています（この表現は、FrustumPNet［100］の入力としても与えられ、分類と回帰のセクションで説明されます）。Dengら[85]のRGB-Dベースの方法は、まずMultiscale Combinatorial Grouping (MCG) [76,138,139] を用いて2D BBを検出する。高速R-CNNベースのアーキテクチャを採用し、x、d、θyを回帰させる。別のRGB-Dベースの2D-driven 3D手法[83]は、Faster R-CNNを用いて2D BB仮説を生成し、L1に基づいて学習された多層パーセプトロン（MLP）を用いてdを回帰させるものである。また、[85]とは異なり、MLPによって生成された仮説は、3次元空間で利用可能な文脈情報（context-info）を利用してさらに改良される。PointFusion [147]は、2次元で検出された物体のRGB画像とLIDAR画像を入力とする。CNNを用いてRGBチャンネルから抽出した特徴と、PointNetを用いて生の深度データから抽出した特徴を融合ネットで結合し、3次元BBを回帰させる

回帰に基づく3Dオブジェクト検出器は、関心のあるオブジェクトの3D BBsを直接推論する。Deep SS [79]は最初の完全な3D領域提案ネットワーク（RPN）を導入し、入力された深度ボリュームを3Dボクセルグリッドに変換し、xとdを回帰させる。両ネットワークはL1s損失に基づいて学習される。PIXOR [146]の提案不要のシングルステージ3D検出法は、ピクセル単位の予測を生成し、その各々が3Dオブジェクト推定に対応する。この方法は、LIDAR点群の鳥瞰図（BEV）表現で動作するため、リアルタイム推論が可能である。VoxelNet [80]は、エンドツーエンドの学習可能なネットとして、特徴学習ネットを介して、疎なLIDAR点群とボクセル化された生の点群を疎な4次元テンソルとして表現するRPNをインターフェースします。この表現を用いて、Faster R-CNNベースのRPNは、L1を用いて学習されたx、d、θyパラメータを回帰させ、3D検出を生成する。MV3D[82]は、LIDARの点群を処理し、BEV画像を取得する。次に、FusionNet[198]は、LIDARのFVとプロポーザルのRGB画像を融合し、3Dボックスコーナー{p3Di}i=1 8を回帰した最終検出値を生成する。Liang ら[143]は、LIDAR と RGB 情報の両方の利点を生かしたエンドツーエンド の学習可能なもう一つの手法である。これは、BEV 特徴マップに RGB 特徴を融合し、パラメータ x、d、θy を回帰させる。Aggregate View Object Detection (AVOD) net [108] は、LIDARの点群とRGB画像を用いて、2つのサブネットで共有される特徴を生成するものである。最初のサブネットである RPN は、パラメータ x と d を回帰して 3D BB の提案を生成し、この提案は、{p2Di}i=1 4 , h1, h2, および θy を回帰して提案をさらに洗練させる 2 番目のサブネットに供給される。

回帰に基づく6次元物体姿勢推定器は、完全な6次元姿勢パラメータを推定するように設計されている。BB8 [40]は、まず、RGB画像中の注目物体をセグメント化し、その領域をCNNに送り込み、3次元BBの8角の投影を予測するように学習させる。完全な6次元の姿勢は、2次元-3次元の対応関係をPnPアルゴリズムに適用して解決される。この方法は、さらに、対称性の問題に対処し、学習サンプルの対称軸周りの回転範囲を0から対称角度γまで狭める。 Tekinら[38]は、オブジェクトの6次元姿勢を推定するために同様のアプローチに従っているが、BB8とは異なり、セグメンテーションを採用せず、p2Dcと共に{p3Di proj}i=1 8を直接回帰している。Oberwegerら[163]は、オクルージョンを処理するために設計された方法を提示している。この手法は、オブジェクトを中心とした画像領域から複数のパッチを抽出し、3D点の2D投影に対するヒートマップを予測する。このヒートマップは、PnPアルゴリズムを用いて集約され、6次元ポーズが推定される。CDPN [167]は、並進と回転の推定を分離し、すべての画像ピクセルに対して別々に3D座標を予測し、PnPを介して予測された座標から回転パラメータを間接的に計算します。Pix2Pose[168]は、この問題の課題である、i)オクルージョン：ピクセルごとの3D座標の推定と生成的敵対学習、ii)対称性：新しいL1ベースの変換損失の導入、iii)精密3Dオブジェクトモデルの欠如：学習段階でテクスチャモデルのないRGB画像の使用、に対処しています。予測された3次元座標に対して、PnPとRANSACを用いて完全な6次元姿勢を求めます。PVNet [160]は、深刻なオクルージョンや切り詰めの状況下で、関心のあるオブジェクトの完全な6D姿勢を推定します。この目的のために、まず、ピクセルからのベクトルをBBのコーナーの投影に回帰し、次にコーナー位置の投票を行う。最後に，EPnP & Levenberg-Marquardt Algorithm (LMA)により，6次元の姿勢を計算する．IHF [31], Sahin et al. [32], LCHF [4], Doumanoglou et al. [35] はオフセットとポーズエントロピー関数でハフフォレスト (HF) を学習し、6次元ポーズパラメータを直接回帰しています。IHF [31]とSahinら[32]は深度画像で動作するように設計されていますが，LCHF [4]とDoumanoglouら[35]はRGB-Dを入力として取ります．6次元のポーズパラメータは、共同学習スキーム[31,32,4]とジョイントレジストレーション（joint reg）[35]を用いてさらに精緻化される。2次元駆動型3D法、3D法、[40,38,161]は実データで学習し、[31,32,4,35]は合成レンダリングで学習している。[163,167,168,159,160] は，実画像と合成画像の両方を用いて学習している．2次元駆動3次元法と3次元BB検出器はカテゴリレベルで、6次元法[4,31,32,35,38,40,159-161,163,167,168]はインスタンスレベルで作業している。

## 分類と回帰

図3は、分類と回帰に基づく手法の全体的な模式図である。分類ベースと回帰ベースという以前のカテゴリーの手法とは異なり、このカテゴリーは分類と回帰のタスクを単一のアーキテクチャで実行する。これらの手法は、まず分類を行い、その結果を回帰ベースの洗練ステップで治す[105,84,78,166]、またはその逆[75]、あるいは分類と回帰をシングルショットで行う[87,145,101,106,100,148,103,102,30,37,162]、ことが可能です。表4は、分類と回帰に基づく方法について説明したものである。Mousavianら[87]は、MS-CNN[123]を修正し、関心対象の2次元BBに加えて、パラメータdとθyが回帰されるようにしたものである。dの回帰はL2損失によって行われ、ビンベースの離散連続損失は、まずθyをn個の重複するビンに離散化し、次に各ビン内の角度を回帰させるために適用される。MonoPSR [106]の入力はRGB画像であり、何の前処理も施されていない。MS-CNN [123]を用いて対象物の2次元BB提案が生成されると、MonoPSRは3次元提案を仮定し、これはCNNスコアリング洗練ステップに送られる。RGB-Dに基づく手法であるFrustumPNet [100]は、FPN [119]を用いてRGB画像中の2D BBを検出するものである。これは、PointNets [140,141]と同様に生の点群を処理することができるため、入力RGB-D画像の深度チャンネルを前処理することはない。xの回帰はL1sを用いてネットの学習を行い、dとθyはFaster R-CNNで採用されているL1sのビンベース版を用いて回帰させる。
Chenら[105]の方法は、入力RGB（St）画像を前処理してシーンのボクセル化された点群を得、この表現を用いて、2段階で注目物体の3D BBを検出する。第1段階では、SVMを学習してx、d、θyを分類する3D BB案を生成する。第2段階では、BB提案を採点し、最終的な検出値を生成する。また、Chenら[84]は[105]と同じ戦略を採用し、単眼のRGB画像を入力としている。深度ベースの手法であるDeepContext[78]は2つのCNNから構成される。最初のCNNはLceを用いて訓練され、xとθyを分類する3D BB提案を生成する。この提案は次に2番目のCNNに送られ、CNNはさらにxとdを回帰して最終的な検出値を生成する。RGB-Dベースの手法[75]は、まずサポートベクトルレグレッサ（SVR）を用いてxを回帰し、さらに損失L0／1に基づいて学習されるCRFを用いて改良するものである。PointPillars [103] は、LIDAR の入力深度画像を点状柱（p-pillar）の擬似画像に変換し、この表現を用いて x、d、θy を出力し、関心対象の 3D BB を生成する。PointRCNN [102]は2段階で3次元BBを検出する。まず、LIDARセンサから取得した生の奥行き画像を直接処理し、ボックスのプロポーザルを生成する。第二段階では、生成された提案は、各提案の点を正準座標に変換する別のネットを使用して洗練される。第1ステージのパラメータと損失関数は、最終的な検出結果を生成するために使用される。　Brachmannら[30]は、RANSACと対数尤度アルゴリズムを用いて、3D検出を6D空間に持ち上げるランダムフォレストベース（RF）アーキテクチャを提示している。SSD-6D [37]は、θcvpとθipを同時に分類し、2次元BBのコーナー{p2Di }i=1 4を回帰するものである。ネットの学習はLceとL1sに基づいて行われる．ネットの出力パラメータは、ICPに基づく精密化とカメラの固有値の利用により、6次元空間へ引き上げられる。2次元駆動型3次元手法と3次元手法は、実データを用いて学習させる。DeepContext[78]は、様々な局所的な物体外観を示す部分合成訓練深度画像で学習し、実データを用いて手法の微調整を行う。2D駆動3D法と3D BB検出器はカテゴリレベルで動作し、6D法[30,37,166,162]はインスタンスレベルで動作する。

## Template matching
### 特徴抽出フェーズ
オフラインの特徴抽出フェーズでは、学習データに含まれる3Dポーズ[180,34,41,181]または6Dポーズ[176,179,178,182,177,23,24,25]に注釈を付けたテンプレートをロバストな特徴記述子で表現する。特徴量は、利用可能な形状、ジオメトリ、および外観情報を利用して手動で作成され[176,179,178,182,177,23,2,25]、この分野の最近のパラダイムは、ニューラルネットアーキテクチャを用いてそれらを深く学習することである[180,34,41,181]。
### テストフェーズ
テンプレートベースの手法は，画像Iを入力とし，オンラインテストの段階でスライディングウィンドウを実行する．各スライディングウィンドウは，特徴記述子（手動で作成された特徴，あるいは深い特徴のいずれか）で表現され，メモリに格納されたテンプレートと比較される．マッチングは特徴空間において行われ，各ウィンドウとテンプレートセットの間の距離が計算される．テンプレートのポーズパラメータは，そのテンプレートと最も近い距離を持つウィンドウに割り当てられる．
表5は、テンプレートマッチングの手法の詳細である。Payetら[176]は、前景オブジェクトに属する右輪郭のヒストグラムであるBag of Boundaries（BOB）表現を導入し、問題を、テスト画像内のBOBを形状テンプレートの集合に照合するように定式化します。この表現により、クラッタに対する頑健性が向上する。Ulrichら[179]は、エッジ特徴を用いてテスト画像をテンプレートの集合にマッチングさせる。この方法は、まず離散的な姿勢を推定し、2Dマッチングと対応する3Dカメラ姿勢をLMAアルゴリズムでさらに精緻化する。小西ら[178]は、テンプレートマッチングに基づく手法では、データベース内のテンプレート数が増加すると性能が低下するため、外観の変化に強いPerspectively Cumulated Orientation Feature（PCOF）表現と階層的ポーズツリー（HPT）検索アルゴリズムを用いた手法を提案しています。エッジマッチングに基づくRAPID-LR (Real-time Attitude and PosItion Determination-Linear Regressor) [182] によって初期推定した6次元姿勢パラメータを、エッジ上のHOG特徴を抽出し、RAPID-HOG距離によってテンプレートデータベースとマッチングすることによってさらに精緻化するものです。元々オブジェクトトラッキングのために提案された手法[177]は、オブジェクトが大量に隠されていたり、カメラの視点から外れているときに発生する一時的なトラッキングロスを処理することができます。この手法は、TCLC（Temporally Consistent Local Color）ヒストグラムに基づき、テスト画像とテンプレートデータベースをマッチングさせ、対象物体を検出するものです。Liuら[23]は，奥行き画像を入力とし，奥行きエッジを抽出することにより，単に物体の6次元姿勢を推定している．MTTM [180]では，まず，テスト画像のセグメンテーションマスクを計算し，次に，CNNフレームワーク内でROI上のテンプレートマッチングを行い，ポーズパラメータを四元数で符号化する．Linemod [2]とHodanら[25]は、表面の法線と色の勾配によってテンプレートの各ペアを表現し、[24]はSVMを訓練して重みを学習し、これをAdaBoostに埋め込んでいる。Wohlhartら[34]は，CNNの学習のために，トリプレットロスLtriとペアワイズロスLpairを定義している．Ltriは，2つの異なるオブジェクトからのディスクリプタ間のユークリッド距離を拡大し，同じオブジェクトからのディスクリプタ間のユークリッド距離を，それらのポーズ間の類似性を代表するものにします．Lpairは，ノイズや照明の変化のような気が散るアーチファクトに対して，ディスクリプタをロバストにします．Balntasら[41]は，CNN学習時にポーズを伴うテンプレートを導くことで，LtriとLpairをさらに改善します（Lpose loss）．Zakharovら[181]は，損失関数LtriとLtriに動的マージンを導入し，学習時間の短縮と，精度を落とさずにより小さいサイズの記述子を使用できるようにしました．34,41]と[181]は，RGB-Dに基づく手法であり，合成データと実データに基づいて深い特徴を学習している．
この手法群は，問題を大域的に定式化し，入力画像から抽出されたテンプレートとウィンドウを特徴記述子で全体的に表現するものである．テスト工程におけるオクルージョンやクラッタから生じる物体境界の歪みは、主にこれらの手法の性能を低下させる。また，深度センサーの欠陥（深度値の欠落，測定値のノイズなど）により，深度不連続面における表面表現が損なわれ，手法の性能劣化を招いている．また、テスト中に抽出された特徴量をテンプレートの集合にマッチングさせるため、未見のグランドトゥルースのアノテーションにうまく一般化できないという欠点もある。

## . Point-pair feature matching
図5は、点対特徴マッチング法の全体概略図である。オフライン段階では、ハッシュテーブルに格納された点対特徴（PPF）により、対象物体の3次元モデルのグローバル表現が形成される。オンラインでは、テスト画像から抽出された点と対の特徴をグローバルモデル表現と比較し、そこから生成されるマッチング候補の集合がポーズパラメータに投票する。
このカテゴリーの手法の詳細について、まずDrostらによる最前線の発表[17]に注目し、次にPPFマッチング性能を向上させるために開発されたアプローチを紹介します。法線n1、n2を持つ2点m1、m2が与えられたとき、PPFは以下のように定義されます[17]。PPF mð Þ¼ 1; ; m2 k k dist 2; ∠ Þ n1; ; dist ; ∠ Þ n2; ; dist ; ∠ Þ n1; ; n2 ここで dist = m2 - m1, ∠(a,b) ∈ [0;π] は二つのベクトル a, b 間の角度を表します。モデル M の大域的な記述は、すべての点ペア {(mr, mi)} ∈ M に対して特徴ベクトル PPF を計算して作成され、特徴ベクトル PPF をインデックスとするハッシュ テーブルとして表現される[16,17]。類似の特徴ベクトルは、ハッシュテーブルの同じビンに位置す るようにグループ化される。大域的モデル記述とは、特徴空間からモデルへの写像であるこ とに注意。
まず基準点srefが選択され、次にIに存在する他の全ての点｛si｝が基準点と対になるように計算される。作成された点ペア{(sref,si)}は、グローバルモデル表現に格納されているものと比較される。この比較は特徴空間で行われ、潜在的なマッチングのセットが得られる[14,16,17]。ここで、Ts と Tm は、sref と mr を原点に平行移動し、それらの法線 nr s と nr m を x 軸に回転させる変換行列である[17] 。回転角αが計算されると、ローカル座標(mr,α)が投票される[14,16,17,21,22]。

点対マッチング法の性能を向上させる最初の試みは、新しい特徴を導き出すことであった[15,18,19,20,21,136]。[15]は，レンジデータにおける偽のマッチングを排除するために，記述子を次元，凸性，連結性などの可視性コンテキストで補強している．[18]は、ターゲットオブジェクトの複数のスキャンビューにのみ依存し、PPFのために色情報を利用する。また，方向を持つ境界点や境界線分も利用されている[19]．強度と範囲の両方からの点対記述子は、シーンとモデルの表面とシルエットのオーバーラップを最適化するために、幾何学的エッジ抽出器とともに使用される[20]。

PPFマッチング法は，類似した外観のディストラクタ，オクルージョン，クラッタ，大きな平面，センサノイズのために，性能不足を示す[201]．その性能は、新しいサンプリングと投票戦略によってさらに向上する[22,135,43]。最大マージン学習[22]で学習した重み付き投票関数は、識別コスト関数を最適化することにより識別特徴を選択しランク付けする。[また，ルックアップテーブルの近傍ビンを用いた投票では，センサノイズに対応する．また、[137]では、投票プロセスを並列化することで、実行時間性能に対処している。最近発表されたディープアーキテクチャは、生の点群から局所記述子を学習するもので、PPFでエンコードされ[203]、さらに通常の表現を利用する[204]。

# Datasets and matrics
## Datasets
KITTI [110]，SUN RGB-D [112]，NYU-Depth v2 [111]，PASCAL3D+ [194] は，3D BB 検出器の共通テスト対象データセットで ある（表 6 参照）．KITTIは14,999枚の画像を有し、そのうち7481枚が検出器の学習に用いられ、残りはテストに利用される。また、自動車、歩行者、自転車の3つの識別カテゴリを持ち、ラベル付けされたオブジェクトの総数は80,256個である。SUN RGB-D [112]は、10,335枚のRGB-D画像を持ち、そのうち3784枚はKinect v2、1159枚はIntelRealSenseによって撮影され、残りはNYU-Depth v2 [111], B3DO [185], SUN3D [186] のデータセットから収集したものです。10,335枚のRGB-D画像に対して、約800の物体カテゴリに対して正確な方向を持つ64,595個の3D BBアノテーションがある（表6の2行目）。2017 SUN RGB-D 3D object detection challenge2では、既存の10,335枚の画像とその64,595個の3D BBアノテーションをトレーニングデータとして使用し、新たに取得した3072個のアノテーション付きオブジェクトBBを持つ2860枚の画像を利用して、19のオブジェクトクラスに対する手法の性能をテストしました（表6の3行目）。NYU-Depth v2データセット[111]は1449枚の画像を含み、そのうち795枚がトレーニング用で、残りがテスト用である。894のオブジェクトカテゴリがあるが、評価対象は主に19のオブジェクトカテゴリ（バスタブ、ベッド、本棚、箱、椅子、カウンター、デスク、ドア、ドレッサー、ゴミ箱、ランプ、モニタ、ナイトスタンド、枕、シンク、ソファ、テーブル、テレビ、トイレ）である。PASCAL VOC 2012 [7] の 12 物体カテゴリ（飛行機、自転車、ボート、ボトル、バス、車、椅子、ダイニングテーブル、バイク、ソファ、電車、テレビモニタ）には、PASCAL3D+ [194] で3Dアノテーションが追加されている。各カテゴリごとに ImageNet [6]からさらに画像を追加し、 その結果、合計 30,899 枚の画像にアノテーションが付 加された。表6で詳述したデータセットとは別に、関連する他の3Dオブジェクト検出データセット[188,189,190,191,192,193]について簡単に言及する。RGB-Dオブジェクトデータセット[188]は、クリーンな背景を持つ300の幾何学的に異なるオブジェクトインスタンスを含む51のオブジェクトクラスが含まれる。189]のデータセットは、いくつかのIKEAオブジェクトの3Dアノテーションを持つ室内画像を提供する。車のカテゴリでは、NYC3DCars[190]とEPFL Cars[191]が提示されている。前者はニューヨークの街角を撮影した画像であり、後者は方位角は異なるが仰角と近距離で撮影された20の自動車インスタンスの2299枚の画像を含んでいる。Table-Top Pose データセット [192] はマウス、マグカップ、ホッチキスの 3 カテゴリ 480 画像からなり、[193] は ImageNet のベッド、椅子、ソファ、テーブルの 4 カテゴリ のサブセットに 3D BB をアノテーションしたものである。  


LINEMOD [2], MULT-I [4], OCC [28], BIN-P [35], T-LESS [42] は、6次元姿勢推定器の性能を検証するために最も頻繁に使用されているデータセットである。最近提案された6次元物体姿勢推定のベンチマーク[5]では、これらのデータセットが改良され、3つの新しいデータセット（Rutgers Amazon Picking Challenge (RU-APC) [184], TU Dresden Light (TUD-L), and Toyota Light (TYO-L) ）とともに統一フォーマットで提示される。(TYO-L))を追加しました。これらのデータセットのパラメータを詳述した表 (Table 1, pp. 25, in [5])を、我々の論文では表 7 に再整形し、データセッ トの課題を提示する。を表7に示す。

ビューポイント（VP）＆クラッタ（C）。RU-APC[184]データセットには，様々な視点と散乱した背景の中に興味対象が配置されたテストシーンが含まれている．

VP & C & テクスチャレス（TL）。LINEMOD[2]データセットのテストシーンは、様々な視点、乱雑な背景でテクスチャのないオブジェクトを含む。15のオブジェクトがあり、それぞれについて1100枚以上の実画像が記録されている。オブジェクトの周囲0-360度、0-90度のチルト回転、⊖45度の面内回転、650mm-1150mmのオブジェクト距離のビューを提供しています。

VP & C & TL & オクルージョン (O) & マルチプルインスタンス (MI)。オクルージョンは、オブジェクト検出と6Dポーズ推定のタスクのためにデータセットをより困難にする主な課題の1つです。近距離、遠距離の2D、3Dクラッタに加え、マルチインスタンス（MULT-I）データセット[4]のテストシーケンスには前景オクルージョンと複数のオブジェクトインスタンスが含まれています。合計で、600mm-1200mmの範囲に位置する6種類のオブジェクトの約2000枚の実画像があります。テスト画像は、ポーズ空間において、ヨー角、ロール角、ピッチ角がそれぞれ [0∘ , - 360∘ ]、[-80∘ , - 80∘] で均一に分布するシーケンスを生成するようにサンプリングされる。

VP & C & TL & Severe Occlusion (SO)。オクルージョン、クラッタ、テクスチャのない物体、視点変更などは、最新の6次元物体検出器でもうまく対処できる最もよく知られた課題である。しかし、これらの課題が大量に存在する場合、6次元物体検出器の性能は著しく低下する。オクルージョン（OCC）データセット[28]は、最大で70〜80%のオクルージョン物体を観測できる最も困難なデータセットの一つである。OCCはLINEMODの拡張グラウンドトゥルースアノテーションを含んでおり、LINEMOD[2]データセットの各テストシーンでは、様々なオブジェクトが存在するが、一つのオブジェクトに対するグラウンドトゥルースポーズのみが与えられている。Brachmannら[28]は、1つのシーン（benchvise）の画像を考慮し、8つの追加オブジェクトのポーズをアノテーションしてOCCを形成している。

VP & SC & SO & MI & Bin Picking (BP)。ビンピッキングシナリオでは、関心のあるオブジェクトの複数のインスタンスが任意にビンにストックされ、したがって、オブジェクトは本質的に厳しいオクルージョンと厳しいクラッタにさらされる。ビンピッキング（BIN-P）データセット[35]は、産業環境で見られるこのような課題を反映するために作成されています。このデータセットには、様々な視点での2つのテクスチャオブジェクトの177のテスト画像が含まれている。

VPとCとTLとOとMIとSimilar-Looking Distractors (SLD)。これらのデータセットに含まれる、類似した外観を持つ物体クラスとともに、類似した外観を持つディストラクタは、形状特徴の識別的な選択の欠如を引き起こし、認識システムを強く混乱させる。上記のデータセットとそれに対応する課題とは異なり、T-LESS[42]データセットは特にこの問題に着目している。テーブル上の物体のRGB-D画像は、360度回転して異なる視点から撮影されており、様々な物体の配置によりオクルージョンが発生する。このため，6DoF法では，学習不足のオブジェクト，類似した外観のディストラクタ（平面），および類似した外観のオブジェクトにより，多くの偽陽性が発生し，特に奥行きモダリティ特徴に影響を与える．T-LESSは、30個の産業関連オブジェクトと、20種類のテストシーンからなり、各シーンは504枚のテスト画像から構成されています。

VP & Light (LI)。TUD-Lは、異なる環境・照明条件下での6Dポーズ推定器の性能を測定するための専用データセットです。テスト画像の最初のフレームで、オブジェクトのポーズは、3Dオブジェクトモデルを使用してシーンに手動で整列されます。この初期姿勢は、ICPを使用してシーケンス全体に伝搬されます。

VPとCとLI。TYO-Lデータセットでは、TUD-Lデータセットと同様に、乱雑な背景の中に対象物を配置し、異なる照明条件を特徴としています。この21個の対象物は、5種類の照明の下で、卓上セットアップで撮影されています。

また、完全6次元姿勢推定器の性能測定に利用される新しいデータセットについても簡単に触れておきます。YCB-Videoデータセット[39]は、92個のビデオシーケンスから構成され、そのうち80個はトレーニングに使用され、残りのシーケンスはテストフェーズに使用されます。実画像の総数は約133Kであり、さらに学習に利用するために約80Kの合成レンダリング画像を含んでいます。テスト画像は、YCBデータセット[195,196]から取得した21個のオブジェクトが存在し、大きな画像ノイズ、様々な照明条件、雑然とした背景でのオクルージョンにさらされている。JHUScene-50[197]データセットには、オフィスワークスペース、ロボット操作プラットフォーム、および大型コンテナを含む5つの異なる室内環境が含まれています。各環境は10シーンを含み、各シーンは、高密度に乱雑な複数のオブジェクトインスタンスを持つ100のテストフレームで構成されている。10個のハンドツールオブジェクト、5000枚のテスト画像、そして合計22,520個のラベル付けされたポーズが存在する。6次元物体姿勢回復のためのサンプルデータセットを図6に示す。

# 評価指数
3次元BB推定や完全な6次元姿勢の仮説の正しさを判断するために、いくつかの評価指標が提案されている。

## Metrics for 3D bounding boxes
4.2.1.1. IoU (Intersection over Union) [7]. この指標は元々、2次元空間で動作する手法の性能を評価するために提示されたものである[7]。推定されたBB BとグランドトゥルースBB Bが与えられ、それらが画像軸と一致していると仮定して、交差の面積B∩Bと連合の面積B∪Bを決定し、それらを比較して重複比率ωIoU2Dを出力する：ωIoU2D ¼ B∩B B∪B ð7Þ 式によれば、B∩BとB∪Bは重複していることを示す。(7)によれば、重複率ωIoU2Dが閾値τIoU2D = 0.5以上であれば、予測された箱は正しい（true positive）とみなされる[7]。この指標はさらに3Dボリュームで動作するように拡張され、3D BBsの重なり比率ωIoU3Dを計算する[89]。この拡張版は、3次元BBが重力方向に整列していることを仮定しているが、他の2軸については仮定していない。

4.2.1.2. 平均精度（AP）[7]。これはPrecision／Recall（PR）曲線の形状を描写するものである。回収範囲r = [0 1]を11の等しいレベルの集合に分割し、この集合における平均精度pを求める[7]。AP ¼ 1 11 X r∈f g 0;0:1;:::1 pinterpðÞ ð r 8Þ 各リコールレベルrにおける精度pは、対応するリコールがrを超えるメソッドについて測定した最大精度を取ることによって補間される。

4.2.1.3. 平均方位類似度（AOS）[110]。この指標は、まず、[7]のAP指標を用いて、2次元画像平面における任意の検出器の物体検出性能を測定し、次に、検出と3次元姿勢推定の結合性能を以下のように評価する[110]。AOS ¼ 1 11 X r∈f g 0;0:1;:: 1 max ~r:~r≥ r simð Þr ; ð10Þ ここで、リコール時の姿勢類似度 sim(r) は、正規化（[0. 1 j Dð Þj r X i∈Dð Þr 1 þ cosΔð Þi θ 2 δi: ð11Þ 式において、DðrÞ は、[0...1]のコサイン類似度である。式(11)において，DðrÞ は想起レベル r の全検出物体集合，Δθ (i) は検出物体 i の推定方向と地上真実方向の角度差，δi は検出物体 i が地上真実 BB に割り当てられた場合（50%以上重なる）1，割り当てられなかった場合は δi = 0 とする．これは，1つの物体を説明する複数の検出をペナルティとするものである．

4.2.1.4. 平均視点精度（AVP）[194]。この指標は、[110]と同様に、2次元物体検出と3次元方位検出を併 せて行う検出器の性能を測定するものである。BBの重なりが50%以上で、かつ、視点が正しい場合（離散視点空間では2つの視点ラベルが同じ、連続視点空間では2視点間の距離が閾値より小さい）に限り、推定が正しいと判断される。そして、視点精度曲線（Viewpoint Precision-Recall: VPR）を描き、その下の面積が平均視点精度に等しいとする。

### Metrics for full 6D pose
3次元BB検出器のメトリクスの詳細を説明したら、次に完全な6次元姿勢推定器の手法のメトリクスを紹介する。

4.2.2.1. 並進角度の誤差。この指標は、オブジェクトのモデルから独立しているため、仮説の正しさを以下に従って測定します[17]： i) 基準真実と推定並進 x および x 間の L2 ノルム、 ii) 基準真実と推定回転行列 R および R の軸角表現から計算される角度： ωTE ¼ k kð x-x 2; ð12Þ ωRE ¼ arccos Tr RR-1 -1 =2 : ð13Þ 式によると、仮説は、スコアωTE および ωRE があらかじめ定義された閾値τTE および 閾値RR-1 以下であれば、正しいと判断されます。(12)および(13)によれば、スコアωTE およびωRE があらかじめ定義された閾値τTE およびτRE よりも小さい場合、仮説は正しいとみなされる。

4.2.2.2. 2D投影。この指標は，注目物体の3次元モデルを入力とし，モデルの頂点vをGround Truthと推定姿勢の両方で画像平面に投影する．対応する頂点の投影の距離は以下のように決定される。ここで、V は物体モデル頂点の集合、K はカメラ行列である。同次座標は L2 ノルムを計算する前に正規化される。平均再投影誤差ω2Dprojが5px以下であれば、推定されたポーズは正しいものとして認められます。この誤差は基本的に対象物体の3Dモデルを用いて計算されるため、点群を使って計算することも可能です。

4.2.2.3. 平均距離(AD)。これは、文献上最も広く使用されているメトリクスの1つである [2]。この指標は、注目物体 O のグランドトゥルース ðx; ; ΘÞ と推定ポーズ (x,Θ) が与えられたとき、 ðx; ; ΘÞ と (x,Θ) 間の AD のスコアである ωAD を出力する。ここで，R と T は真実のポーズ ðx; ; ΘÞ の回転行列と並進行列を，R と T は推定ポーズ (x,Θ) の回転行列と並進行列を表す．ここで、Φは3次元モデルMの直径、zωは正解とする仮説の粗密を決める定数である。ただし、式（15）は、モデルが曖昧でない、あるいは曖昧と思われるビューの部分集合を持たない物体に対して有効である。ここで、ωADは最も近いモデル点へのADとして計算される。この関数は多対一の点マッチングを採用し、対称な物体やオクルーデッド物体を著しく促進し、より低いωADスコアを生成する。

4.2.2.4. 可視面不一致（VSD）。このメトリックは、オブジェクトの対称性とオクルージョンから生じる曖昧さを排除するために最近提案されたものである[3]。本手法は，注目物体 O のモデル M をグランドトゥルース ðx; ; ΘÞ と推定 (x,Θ) の両姿勢でレンダリングし，レンダリング の深度マップ D, D をテスト深度画像 ID 自体と交差させて可視 マスク V, V を生成し，その比較から，推定が正しいかどうかを 事前定義閾値 τに従って決定するスコア ωVSD が以下のように計算され る．ωVSD ¼ avg q∈V∪V 0 if q∈V∩V∧j Dð Þ q -Dð Þj q bτ 1 otherwise 8 < : ð18Þ ここで、q は画素を表す。このように、スコアωVSDはモデル表面の可視部分のみに対して計算されるため、区別できないポーズは等価として扱われる。

4.2.2.5. Sym pose distance. VSD [3]は、曖昧さ不変のポーズエラー関数として提示されます。あるポーズ仮説が正しいものとして受け入れられるかどうかは、利用可能なデータから推定されるポーズの信憑性に依存します。しかし、多くのアプリケーション、特にアクティブビジョンの文脈では、正確な姿勢推定に依存しており、尤もらしい仮説では満足できないため、VSDは評価上問題があると考えられている。また、対象物の隠蔽性が高い場合、尤もらしい仮説の数は無限大になる可能性がある。Sym pose distanceは、対称性の問題から生じる曖昧さを、理論的な裏付けのある枠組みで取り扱う。この枠組みにおいて，この指標はポーズP1とP2の間のスコアωSymを出力する．ωSym ¼ min G1; G2∈G ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 A Z A k W2∘G2ð Þ x -W1∘G1ð Þ x 2 da; s ð19Þ ここで、ポーズ P1 は同値クラス {W1 ∘ G1 (x)｜G1∈G} に、ポーズ P2 は同値クラス {W2 ∘ G2(x)｜G2∈G} に対応するものであるとする。Aは対象物の表面積、W1、W2は剛体変換、G1、G2はG∈SE(3)の対称群である。

4.2.2.6. 実装の詳細 本研究では、ADとVSDの両メトリクスを用いた6D検出器の2重の評価戦略を採用する：i）Recall. 各オブジェクトのテスト画像上の仮説をランク付けし，最も重みのある仮説を推定6Dポーズとして選択する。リコールの値は、正しく推定されたポーズの数と、関心のあるオブジェクトのテスト画像の数を比較して計算される。再現率と異なり、全ての仮説が考慮され、精度と再現率の調和平均であるF1スコアが提示される。